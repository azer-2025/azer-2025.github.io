[{"id":"5c0d008f6ea2400e1b86324b9faa98ca","title":"Prompt 工程","content":"1、Prompt 工程​\t\t很多人其实根本不了解Prompt ,这里我自己的最简单的理解是：我自己使用ai的时候，我会先给他一个角色，比如你现在只能用SpringBoot框架语言帮我写，或者是你是一个大文豪诗人，这就是最简单的Prompt，根据你的需求，可以更加细化它自身的设定是个什么东西。\n1、基本概念Prompt ⁠工程（Prompt Eng‌ineering）又叫提示词工程，简单来说，就是输入‎给 AI 的指令。比如下面‌这段内容，就是提示词：\n1请问Azer的Blog怎么样？\n\n那为什么要叫 “工程” 呢？\n因为 AI 大模型生成⁠的内容是不确定的，构建一个能够按照预期生成‌内容的提示词既是一门艺术，也是一门科学。提示词的质量直接影响到 AI 大模型输出的结‎果，因此这也是 AI 应用开发的关键技能，‌很多公司专门招聘提示词工程师。\n我们学习 ⁠Prompt 工程‌的目标是：通过精心设计和优化输入提示‎来引导 AI 模型‌生成符合预期的高质量输出。\n提示词分类核心 - 基于角色的分类在 AI ⁠对话中，基于角色的‌分类是最常见的，通常存在 3 种主要‎类型的 Promp‌t：\n**1）用户 Promp⁠t (User Prompt)：**这是用户‌向 AI 提供的实际问题、指令或信息，传达了用户的直接需求。用户 Prompt ‎告诉 AI 模型 “做什么”，比如回答问‌题、编写代码、生成创意内容等。\n1用户：帮我写一首关于春天的短诗\n\n**2）系统 Prompt⁠ (System Prompt)：**这是设置‌ AI 模型行为规则和角色定位的隐藏指令，用户通常不能直接看到。系统 Prompt ‎相当于给 AI 设定人格和能力边界，即告诉‌ AI “你是谁？你能做什么？”。\n1系统：你是一位经验丰富的恋爱顾问，擅长分析情感问题并提供建设性建议。请以温暖友善的语气回答用户的恋爱困惑，必要时主动询问更多信息以便提供更准确的建议。不要做出道德判断，而是尊重用户的情感体验并提供实用的沟通和相处技巧。回答时保持专业性，但避免使用过于学术的术语，确保普通用户能够理解你的建议。\n\n不同的系统 P⁠rompt 可以让同一个 ‌AI 模型表现出完全不同的应用特性，这是构建垂直领域‎ AI 应用（如财务顾问、‌教育辅导、医疗咨询等）的关键。\n比如 23 ⁠年 AI 刚流行的时候‌，很多 AI 助手平台，都是基于设置不同的系‎统 Prompt 来提‌供不同的 AI 助手。\n**3）助手 Prompt ⁠(Assistant Prompt)：**这是 AI‌ 模型的响应内容。在多轮对话中，之前的助手回复也会成为当前上下文的一部分，影响后续对话的理解‎和生成。某些场景下，开发者可以主动预设一些助手消息作‌为对话历史的一部分，引导后续互动。\n1助手：我是你的恋爱顾问，很高兴能帮助你解决情感问题。你目前遇到了什么样的恋爱困惑呢？可以告诉我你们的关系现状和具体遇到的问题吗？\n\n在实际应用⁠中，这些不同类型的‌提示词往往会组合使用。举个例子，一个‎完整的对话可能包含‌：\n1234567系统：你是专业编程导师，擅长引导初学者入门编程并制定学习路径。使用友好鼓励的语气，解释复杂概念时要通俗易懂，适当使用比喻让新手理解，避免过于晦涩的技术术语。用户：我完全没有编程基础，想学习编程开发，但不知道从何开始，能给我一些建议吗？助手：欢迎加入编程的世界！作为编程小白，建议你可以按照以下步骤开始学习之旅...【多轮对话继续】\n\nAI 大模⁠型开发平台允许用户‌自主设置各种不同类型的提示词来进行调‎试：\n\n2、TokenToken 是⁠大模型处理文本的基本单位，可‌能是单词或标点符号，模型的输入和输出都是按 Token ‎计算的，一般 Token 越‌多，成本越高，并且输出速度越慢。\n因此在 A⁠I 应用开发中，了‌解和控制 Token 的消耗至关重要‎。\n如何计算 Token？首先，不同⁠大模型对 Toke‌n 的划分规则略有不同，比如根据 O‎penAI 的文档‌：\n\n英文文本：一个 token 大约相当于 4 个字符或约 0.75 个英文单词\n中文文本：一个汉字通常会被编码为 1-2 个 token\n空格和标点：也会计入 token 数量\n特殊符号和表情符号：可能需要多个 token 来表示\n\n简单估算一下⁠，100 个英文单词约等‌于 75-150 个 Token，而 100 个‎中文字符约等于 100-‌200 个 Token。\n实际应用中⁠，更推荐使用工具来‌估计 Prompt 的 Token ‎数量，比如：\n\nOpenAI Tokenizer：适用于 OpenAI 模型的官方 Token 计算器\n非官方的 Token 计算器\n\nToken 成本计算估算成本有⁠个公式：总成本 &#x3D;‌ (输入 token数 × 输入单价)‎ + (输出 tok‌en 数 × 输出单价)\n不同大模型⁠的计费都不太一样，‌因此要认真阅读官方文档的计费标准，比‎如阿里系大模型：\n\n建议大家估⁠算成本时，可以多去‌对比不同大模型的价格，参考下列表格去‎整理一个详细的对比‌表格，结果一目了然：\n\n\n\n模型\n输入价格 (&#x2F;1K tokens)\n输出价格 (&#x2F;1K tokens)\n1000 字对话预估成本\n\n\n\nGPT-xx\n$0.0015\n$0.002\n¥0.02-0.03\n\n\nGPT-xxx ⁠\n$0.03\n$0.06‌\n¥0.3-0.5\n\n\nClaude-xxx\n$0.00‎025\n$0.00125 ‌\n¥0.01-0.02\n\n\nToken 成本优化技巧注意，系统⁠提示词、用户提示词‌和 AI 大模型输出的内容都是消耗成‎本的，因此我们成本‌优化主要从这些角度进行。\n1）精简系统⁠提示词：移除冗余表述，保‌留核心指令。比如将 “你是一个非常专业、经验丰富‎且非常有耐心的编程导师”‌ 简化为 “你是编程导师”。\n2）定期清理⁠对话历史：对话上下文会随‌着交互不断累积 Token。在长对话中，可以定期‎请求 AI 总结之前的对‌话，然后以总结替代详细历史。\n1请总结我们至今的对话要点，后续我们将基于此总结继续讨论。\n\n3）使用向量检索⁠代替直接输入：对于需要处理大量参‌考文档的场景，不要直接将整个文档作为 Prompt，而是使用向量‎数据库和检索技术（RAG）获取‌相关段落。后续教程会带大家实战。\n4）结构化⁠替代自然语言：使用‌表格、列表等结构化格式代替长段落描述‎。\n举个例子，优化前：\n1请问如何制作披萨？首先需要准备面粉、酵母、水、盐、橄榄油作为基础面团材料。然后根据口味选择酱料，可以是番茄酱或白酱。接着准备奶酪，最常用的是马苏里拉奶酪。最后准备各种配料如意大利香肠、蘑菇、青椒等。\n\n优化后：\n1234567披萨制作材料：- 面团：面粉、酵母、水、盐、橄榄油- 酱料：番茄酱/白酱- 奶酪：马苏里拉- 配料：意大利香肠、蘑菇、青椒等如何制作？\n\n2、Prompt 优化技巧前面也提到了，⁠设计 Prompt 是一门‌艺术，高质量的 Prompt 可以显著提升 AI 输‎出的质量，因此我们需要重点‌掌握 Prompt 优化技巧。\n利用资源1、Prompt 学习网上和 Pro⁠mpt 优化相关的资源非常‌丰富，几乎各大主流 AI 大模型和 AI 开发框架官‎方文档都有相关的介绍，推荐‌先阅读至少 2 篇，比如：\n\nPrompt Engineering Guide 提示工程指南\nOpenAI 提示词工程指南\nSpring AI 提示工程指南\nAuthropic 提示词工程指南\nAuthropic 提示词工程指南（开源仓库）\n智谱 AI Prompt 设计指南\n\n2、Prompt 提示词库网上也有很⁠多现成的提示词库，‌在自主优化提示词前，可以先尝试搜索有‎没有现成的提示词参‌考：\n\n文本对话：Authropic 提示词库\nAI 绘画：Midjourney 提示词库\n\n基础提示技巧1、明确指定任务和角色为 AI ⁠提供清晰的任务描述‌和角色定位，帮助模型理解背景和期望。\n12系统：你是一位经验丰富的Python教师，擅长向初学者解释编程概念。用户：请解释 Python 中的列表推导式，包括基本语法和 2-3 个实用示例。\n\n2、提供详细说明和具体示例提供足够的⁠上下文信息和期望的‌输出格式示例，减少模型的不确定性。\n12345678910请提供一个社交媒体营销计划，针对一款新上市的智能手表。计划应包含:1. 目标受众描述2. 三个内容主题3. 每个平台的内容类型建议4. 发布频率建议示例格式:目标受众: [描述]内容主题: [主题1], [主题2], [主题3]平台策略: [平台] - [内容类型] - [频率]\n\n3、使用结构化格式引导思维通过列表、表格等结构化格式，使指令更易理解，输出更有条理。\n123456789分析以下公司的优势和劣势:公司: Tesla请使用表格格式回答，包含以下列:- 优势(最少3项)- 每项优势的简要分析- 劣势(最少3项)- 每项劣势的简要分析- 应对建议\n\n4、明确输出格式要求指定输出的格式、长度、风格等要求，获得更符合预期的结果。\n12345撰写一篇关于气候变化的科普文章，要求:- 使用通俗易懂的语言，适合高中生阅读- 包含5个小标题，每个标题下2-3段文字- 总字数控制在800字左右- 结尾提供3个可行的个人行动建议\n\n进阶提示技巧⁠1、思维链提示法（‌Chain-of-Thought）引导模型展示推理过程，逐步思考问题，提高复杂问题的准确性。\n1234567问题：一个商店售卖T恤，每件15元。如果购买5件以上可以享受8折优惠。小明买了7件T恤，他需要支付多少钱？请一步步思考解决这个问题:1. 首先计算7件T恤的原价2. 确定是否符合折扣条件3. 如果符合，计算折扣后的价格4. 得出最终支付金额\n\n⁠2、少样本学习（F‌ew-Shot Learning）通过提供几⁠个输入 - 输出对的示‌例，帮助模型理解任务模式和期望输出。\n12345678910我将给你一些情感分析的例子，然后请你按照同样的方式分析新句子的情感倾向。输入: &quot;这家餐厅的服务太差了，等了一个小时才上菜&quot;输出: 负面，因为描述了长时间等待和差评服务输入: &quot;新买的手机屏幕清晰，电池也很耐用&quot;输出: 正面，因为赞扬了产品的多个方面现在分析这个句子:&quot;这本书内容还行，但是价格有点贵&quot;\n\n3、分步骤指导（Step-by-Step）将复杂任务分解为可管理的步骤，确保模型完成每个关键环节。\n1234567请帮我创建一个简单的网站落地页设计方案，按照以下步骤:步骤1: 分析目标受众(考虑年龄、职业、需求等因素)步骤2: 确定页面核心信息(主标题、副标题、价值主张)步骤3: 设计页面结构(至少包含哪些区块)步骤4: 制定视觉引导策略(颜色、图像建议)步骤5: 设计行动召唤(CTA)按钮和文案\n\n4、自我评估和修正让模型评估自己的输出并进行改进，提高准确性和质量。\n12345678解决以下概率问题:从一副标准扑克牌中随机抽取两张牌，求抽到至少一张红桃的概率。首先给出你的解答，然后:1. 检查你的推理过程是否存在逻辑错误2. 验证你使用的概率公式是否正确3. 检查计算步骤是否有误4. 如果发现任何问题，提供修正后的解答\n\n5、知识检索和引用引导模型检索相关信息并明确引用信息来源，提高可靠性。\n1234567请解释光合作用的过程及其在植物生长中的作用。在回答中:1. 提供光合作用的科学定义2. 解释主要的化学反应3. 描述影响光合作用效率的关键因素4. 说明其对生态系统的重要性对于任何可能需要具体数据或研究支持的陈述，请明确指出这些信息的来源，并说明这些信息的可靠性。\n\n6、多视角分析引导模型从不同角度、立场或专业视角分析问题，提供全面见解。\n123456789101112分析&quot;城市应该禁止私家车进入市中心&quot;这一提议:请从以下4个不同角度分析:1. 环保专家视角2. 经济学家视角3. 市中心商户视角4. 通勤居民视角对每个视角:- 提供支持该提议的2个论点- 提供反对该提议的2个论点- 分析可能的折中方案\n\n7、多模态思维结合不同表⁠达形式进行思考，如‌文字描述、图表结构、代码逻辑等。\n12345678设计一个智能家居系统的基础架构:1. 首先用文字描述系统的主要功能和组件2. 然后创建一个系统架构图(用ASCII或文本形式表示)3. 接着提供用户交互流程4. 最后简述实现这个系统可能面临的技术挑战尝试从不同角度思考:功能性、用户体验、技术实现、安全性等。\n\n提示词调试与优化好的提示词⁠可能很难一步到位，‌因此我们要学会如何持续调试和优化 Pro‎mpt。\n1、迭代式提示优化通过逐步修改和完善提示词，提高输出质量。\n1234567891011初始提示: 谈谈人工智能的影响。[收到笼统回答后]改进提示: 分析人工智能对医疗行业的三大积极影响和两大潜在风险，提供具体应用案例。[如果回答仍然不够具体]进一步改进: 详细分析AI在医学影像诊断领域的具体应用，包括:1. 现有的2-3个成功商业化AI诊断系统及其准确率2. 这些系统如何辅助放射科医生工作3. 实施过程中遇到的主要挑战4. 未来3-5年可能的技术发展方向\n\n2、边界测试通过极限情况测试模型的能力边界，找出优化空间。\n1234567尝试解决以下具有挑战性的数学问题:证明在三角形中，三条高的交点、三条中线的交点和三条角平分线的交点在同一条直线上。如果你发现难以直接证明:1. 说明你遇到的具体困难2. 考虑是否有更简单的方法或特例可以探讨3. 提供一个思路框架，即使无法给出完整证明\n\n3、提示词模板化创建结构化⁠模板，便于针对类似‌任务进行一致性提示，否则每次输出的内‎容可能会有比较大的‌区别，不利于调试。\n1234567891011121314151617181920【专家角色】: &#123;领域&#125;专家【任务描述】: &#123;任务详细说明&#125;【所需内容】:- &#123;要点1&#125;- &#123;要点2&#125;- &#123;要点3&#125;【输出格式】: &#123;格式要求&#125;【语言风格】: &#123;风格要求&#125;【限制条件】: &#123;字数、时间或其他限制&#125;例如:【专家角色】: 营养学专家【任务描述】: 为一位想减重的上班族设计一周健康饮食计划【所需内容】:- 七天的三餐安排- 每餐的大致卡路里- 准备建议和购物清单【输出格式】: 按日分段，每餐列出具体食物【语言风格】: 专业但友好【限制条件】: 考虑准备时间短，预算有限\n\n4、错误分析与修正系统性分析⁠模型回答中的错误，并‌针对性优化提示词，这一点在我们使用 Cu‎rsor 等 AI ‌开发工具生成代码时非常有用。\n12345678910我发现之前请你生成的Python代码存在以下问题:1. 没有正确处理文件不存在的情况2. 数据处理逻辑中存在边界条件错误3. 代码注释不够详细请重新生成代码，特别注意:1. 添加完整的异常处理2. 测试并确保所有边界条件3. 为每个主要函数和复杂逻辑添加详细注释4. 遵循PEP 8编码规范\n\n虽然前面提到了这么多提示词优化技巧，但总结出来就一句话：任务越复杂，就越要给 Prompt 补充更多细节。\nMVP 最小可行产品策略MVP 最小可行产品策略是指先开发包含 核心功能 的基础版本产品快速推向市场，以最小成本验证产品假设和用户需求。通过收集真实用户反馈进行迭代优化，避免开发无人使用的功能，降低资源浪费和开发风险。\n基于这个策略，我们可以先⁠开发一个简单但实用的 AI 对话应用，让用户能‌够和 AI 恋爱大师进行多轮对话交流。因为 “对话” 是本产品的核心功能，暂时不要考虑更复杂的‎功能了。后续可以根据用户用量和反馈，决定下一‌步是深化对话能力还是扩展更多功能模块。\n明确需求后，下面我们进行方案设计，看看怎么实现这个需求。\n3、AI 应用方案设计根据需求，⁠我们将实现一个具有‌多轮对话能力的 AI 恋爱大师应用。‎整体方案设计将围绕‌ 2 个核心展开：\n\n系统提示词的设计\n多轮对话的实现\n\n1、系统提示词设计前面提到，⁠系统提示词相当于 ‌AI 应用的 “灵魂”，直接决定了 ‎AI 的行为模式、‌专业性和交互风格。\n对于 AI⁠ 对话应用，最简单‌的做法是直接写一段系统预设，定义 “‎你是谁？能做什么？‌”，比如：\n1你是一位恋爱大师，为用户提供情感咨询服务\n\n这种简单提示虽然⁠可以工作，但效果往往不够理想。想‌想现实中的场景，我们去找专家咨询时，专家可能会先主动抛出一系列引‎导性问题、深入了解背景，而不是被‌动等待用户完整描述问题。比如：\n\n最近有什么迷茫的事情么？\n请问你有什么需要我帮助的事么？\n你们的感情遇到什么问题了么？\n\n用户会跟 AI⁠ 进行多轮对话，这时 AI‌ 不能像失忆一样，而是要始终保持之前的对话内容作为上‎下文，不断深入了解用户，从‌而提供给用户更全面的建议。\n因此我们要⁠优化系统预设，可以‌借助 AI 进行优化。示例 Prom‎pt：\n1我正在开发【恋爱大师】AI 对话应用，请你帮我编写设置给 AI 大模型的系统预设 Prompt 指令。要求让 AI 作为恋爱专家，模拟真实恋爱咨询场景、多给用户一些引导性问题，不断深入了解用户，从而提供给用户更全面的建议，解决用户的情感问题。\n\nAI 提供的优化后系统提示词：\n1扮演深耕恋爱心理领域的专家。开场向用户表明身份，告知用户可倾诉恋爱难题。围绕单身、恋爱、已婚三种状态提问：单身状态询问社交圈拓展及追求心仪对象的困扰；恋爱状态询问沟通、习惯差异引发\n\n2、多轮对话实现要实现具有 “记忆力” 的 AI 应用，让 AI 能够记住用户之前的对话内容并保持上下文连贯性，我们可以使用 Spring AI 框架的 对话记忆能力。\n如何使用对话记忆能力呢？参考 Spring AI 的官方文档，了解到 Spring AI 提供了 ChatClient API 来和 AI 大模型交互。\nChatClient特性之前我们是直接使用 Spring Boot 注入的 ChatModel 来调用大模型完成对话，而通过我们自己构造的 ChatClient，可实现功能更丰富、更灵活的 AI 对话客户端，也更推荐通过这种方式调用 AI。\n通过示例代码，⁠能够感受到 ChatMode‌l 和 ChatClient 的区别。ChatClien‎t 支持更复杂灵活的链式调用‌（Fluent API）：\n1234567ChatResponse response = chatModel.call(new Prompt(&quot;你好&quot;));ChatClient chatClient = ChatClient.builder(chatModel)    .defaultSystem(&quot;你是恋爱顾问&quot;)    .build();    String response = chatClient.prompt().user(&quot;你好&quot;).call().content();\n\nSprin⁠g AI 提供了多‌种构建 ChatClient 的方式‎，比如自动注入、通‌过建造者模式手动构造：\n123456789101112131415@Servicepublic class ChatService &#123;    private final ChatClient chatClient;        public ChatService(ChatClient.Builder builder) &#123;        this.chatClient = builder            .defaultSystem(&quot;你是恋爱顾问&quot;)            .build();    &#125;&#125;ChatClient chatClient = ChatClient.builder(chatModel)    .defaultSystem(&quot;你是恋爱顾问&quot;)    .build();\n\nChatC⁠lient 支持多‌种响应格式，比如返回 ChatRes‎ponse 对象、‌返回实体对象、流式返回：\n12345678910111213141516171819202122232425ChatResponse chatResponse = chatClient.prompt()    .user(&quot;Tell me a joke&quot;)    .call()    .chatResponse();record ActorFilms(String actor, List&lt;String&gt; movies) &#123;&#125;ActorFilms actorFilms = chatClient.prompt()    .user(&quot;Generate the filmography for a random actor.&quot;)    .call()    .entity(ActorFilms.class);List&lt;ActorFilms&gt; multipleActors = chatClient.prompt()    .user(&quot;Generate filmography for Tom Hanks and Bill Murray.&quot;)    .call()    .entity(new ParameterizedTypeReference&lt;List&lt;ActorFilms&gt;&gt;() &#123;&#125;);Flux&lt;String&gt; streamResponse = chatClient.prompt()    .user(&quot;Tell me a story&quot;)    .stream()    .content();Flux&lt;ChatResponse&gt; streamWithMetadata = chatClient.prompt()    .user(&quot;Tell me a story&quot;)    .stream()    .chatResponse();\n\n可以给 C⁠hatClient ‌设置默认参数，比如系统提示词，还可以在对‎话时动态更改系统提示‌词的变量，类似模板的概念：\n12345678910ChatClient chatClient = ChatClient.builder(chatModel)        .defaultSystem(&quot;You are a friendly chat bot that answers question in the voice of a &#123;voice&#125;&quot;)        .build();chatClient.prompt()        .system(sp -&gt; sp.param(&quot;voice&quot;, voice))        .user(message)        .call()        .content());\n\n此外，还支⁠持指定默认对话选项‌、默认拦截器、默认函数调用等等，后面‎教程中都会用到。\nAdvisorsSpring AI 使用 Advisors（顾问）机制来增强 AI 的能力，可以理解为一系列可插拔的拦截器，在调用 AI 前和调用 AI 后可以执行一些额外的操作，比如：\n\n前置增强：调用 AI 前改写一下 Prompt 提示词、检查一下提示词是否安全\n后置增强：调用 AI 后记录一下日志、处理一下返回的结果\n\n为了便于大家理解，后续教程中我可能会经常叫它为拦截器。\n用法很简单，我们可⁠以直接为 ChatClient 指定‌默认拦截器，比如对话记忆拦截器 MessageChatMemoryAdv‎isor 可以帮助我们实现多轮对话能‌力，省去了自己维护对话列表的麻烦。\n1234567891011121314var chatClient = ChatClient.builder(chatModel)    .defaultAdvisors(        new MessageChatMemoryAdvisor(chatMemory),         new QuestionAnswerAdvisor(vectorStore)        )    .build();String response = this.chatClient.prompt()        .advisors(advisor -&gt; advisor.param(&quot;chat_memory_conversation_id&quot;, &quot;678&quot;)            .param(&quot;chat_memory_response_size&quot;, 100))    .user(userText)    .call()\t.content();\n\nAdvisors 的原理图如下：\n\n解释上图的执行流程：\n\nSpring AI 框架从用户的 Prompt 创建一个 AdvisedRequest，同时创建一个空的 AdvisorContext 对象，用于传递信息。\n链中的每个 advisor 处理这个请求，可能会对其进行修改。或者，它也可以选择不调用下一个实体来阻止请求继续传递，这时该 advisor 负责填充响应内容。\n由框架提供的最终 advisor 将请求发送给聊天模型 ChatModel。\n聊天模型的响应随后通过 advisor 链传回，并被转换为 AdvisedResponse。后者包含了共享的 AdvisorContext 实例。\n每个 advisor 都可以处理或修改这个响应。\n最终的 AdvisedResponse 通过提取 ChatCompletion 返回给客户端。\n\n实际开发中，往往我们会用到多个拦截器，组合在一起相当于一条拦截器链条（责任链模式的设计思想）。每个拦截器是有顺序的，通过 getOrder() 方法获取到顺序，得到的值越低，越优先执行。\n比如下面的代码中，如果单独⁠按照代码顺序，可能我们会认为：将首先执行 Messa‌geChatMemoryAdvisor，将对话历史记录添加到提示词中。然后，QuestionAnswer‎Advisor 将根据用户的问题和添加的对话历史记录‌执行知识库检索，从而提供更相关的结果：\n123456var chatClient = ChatClient.builder(chatModel)    .defaultAdvisors(        new MessageChatMemoryAdvisor(chatMemory),         new QuestionAnswerAdvisor(vectorStore)        )    .build();\n\n但是实际上⁠，我们拦截器的执行‌顺序是由 getOrder 方法决定‎的，不是简单地根据‌代码的编写顺序决定。\nAdvisor 类图如下，了解即可：\n\n从上图中我们发现，Advi⁠sors 分为 2 种模式：流式 Streamin‌g 和非流式 Non-Streaming，二者在用法上没有明显的区别，返回值不同罢了。但是如果我们要‎自主实现 Advisors，为了保证通用性，最好还‌是同时实现流式和非流式的环绕通知方法。\n\nChat Memory Advisor前面我们提到⁠了，想要实现对话记忆功能‌，可以使用 Spring AI 的 ChatMe‎moryAdvisor，‌它主要有几种内置的实现方式：\n\nMessageChatMemoryAdvisor：从记忆中检索历史对话，并将其作为消息集合添加到提示词中\nPromptChatMemoryAdvisor：从记忆中检索历史对话，并将其添加到提示词的系统文本中\nVectorStoreChatMemoryAdvisor：可以用向量数据库来存储检索历史对话\n\nMessag⁠eChatMemoryA‌dvisor 和 PromptChatMemor‎yAdvisor 用法类‌似，但是略有一些区别：\n1）Messag⁠eChatMemoryAdvi‌sor 将对话历史作为一系列独立的消息添加到提示中，保留原始‎对话的完整结构，包括每条消息的‌角色标识（用户、助手、系统）。\n12345[  &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;你好&quot;&#125;,  &#123;&quot;role&quot;: &quot;assistant&quot;, &quot;content&quot;: &quot;你好！有什么我能帮助你的吗？&quot;&#125;,  &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;讲个笑话&quot;&#125;]\n\n2）Prom⁠ptChatMemor‌yAdvisor 将对话历史添加到提示词的系‎统文本部分，因此可能会‌失去原始的消息边界。\n123456以下是之前的对话历史：用户: 你好助手: 你好！有什么我能帮助你的吗？用户: 讲个笑话现在请继续回答用户的问题。\n\n** 一般情况下，更建议使用 MessageChatMemoryAdvisor。** 更符合大多数现代 LLM 的对话模型设计，能更好地保持上下文连贯性。\nChat Memory上述 ChatMemoryAdvisor 都依赖 Chat Memory 进行构造，Chat Memory 负责历史对话的存储，定义了保存消息、查询消息、清空消息历史的方法。\n\nSprin⁠g AI 内置了几‌种 Chat Memory，可以将对‎话保存到不同的数据‌源中，比如：\n\nInMemoryChatMemory：内存存储\nCassandraChatMemory：在 Cassandra 中带有过期时间的持久化存储\nNeo4jChatMemory：在 Neo4j 中没有过期时间限制的持久化存储\nJdbcChatMemory：在 JDBC 中没有过期时间限制的持久化存储\n\n当然也可以⁠通过实现 Chat‌Memory 接口自定义数据源的存储‎，本教程后续会带‌大家实战。\n了解了 S⁠pring AI ‌多轮对话的实现机制后，下面我们进入 ‎AI 应用的开发。\n4、多轮对话 AI 应用开发在后端项目根包下新建 app 包，存放 AI 应用，新建 LoveApp.java。可以参考 Spring AI Alibaba 官方的 示例代码 实现（其实用的还是 Spring AI）。\n1）首先初始化 ChatC⁠lient 对象。使用 Spring 的构造器注入方‌式来注入阿里大模型 dashscopeChatModel 对象，并使用该对象来初始化 ChatCli‎ent。初始化时指定默认的系统 Prompt 和基于内存‌的对话记忆 Advisor。代码如下：\n12345678910111213141516171819202122@Component@Slf4jpublic class LoveApp &#123;    private final ChatClient chatClient;    private static final String SYSTEM_PROMPT = &quot;扮演深耕恋爱心理领域的专家。开场向用户表明身份，告知用户可倾诉恋爱难题。&quot; +            &quot;围绕单身、恋爱、已婚三种状态提问：单身状态询问社交圈拓展及追求心仪对象的困扰；&quot; +            &quot;恋爱状态询问沟通、习惯差异引发的矛盾；已婚状态询问家庭责任与亲属关系处理的问题。&quot; +            &quot;引导用户详述事情经过、对方反应及自身想法，以便给出专属解决方案。&quot;;    public LoveApp(ChatModel dashscopeChatModel) &#123;                ChatMemory chatMemory = new InMemoryChatMemory();        chatClient = ChatClient.builder(dashscopeChatModel)                .defaultSystem(SYSTEM_PROMPT)                .defaultAdvisors(                        new MessageChatMemoryAdvisor(chatMemory)                )                .build();    &#125;&#125;\n\n2）编写对话方法⁠。调用 chatClie‌nt 对象，传入用户 Prompt，并且给 advi‎sor 指定对话 id 和对话‌记忆大小。代码如下：\n123456789101112public String doChat(String message, String chatId) &#123;    ChatResponse response = chatClient            .prompt()            .user(message)            .advisors(spec -&gt; spec.param(CHAT_MEMORY_CONVERSATION_ID_KEY, chatId)                    .param(CHAT_MEMORY_RETRIEVE_SIZE_KEY, 10))            .call()            .chatResponse();    String content = response.getResult().getOutput().getText();    log.info(&quot;content: &#123;&#125;&quot;, content);    return content;&#125;\n\n3）编写单元测试，测试多轮对话：\n1234567891011121314151617181920212223@SpringBootTestclass LoveAppTest &#123;    @Resource    private LoveApp loveApp;    @Test    void testChat() &#123;        String chatId = UUID.randomUUID().toString();                String message = &quot;你好，我是Azer&quot;;        String answer = loveApp.doChat(message, chatId);        Assertions.assertNotNull(answer);                message = &quot;我想让另一半（ZA）更爱我&quot;;        answer = loveApp.doChat(message, chatId);        Assertions.assertNotNull(answer);                message = &quot;我的另一半叫什么来着？刚跟你说过，帮我回忆一下&quot;;        answer = loveApp.doChat(message, chatId);        Assertions.assertNotNull(answer);    &#125;&#125;\n\n\n成功，并且拥有会话记忆\n","slug":"Prompt提示词","date":"2025-11-05T11:14:09.000Z","categories_index":"SpringBoot,Spring AI","tags_index":"Spring AI,大模型,AI","author_index":"Azer"},{"id":"74dcc5cfce20b2ecd28309464beb868c","title":"大模型的接入","content":"大模型的接入0.引言​\t\t最近在学习ai大模型相关的概念，在之前说实话我是一直没学过这方面的相关知识，在2022、2023年左右的时候，那时候ai才刚开始流行，在这之前，主播做任何开发甚至没有用到ai，现在往回看感觉自己像个**，之前做的前端和后端，做的最多的就是b站学习，然后要一份源码，复制粘贴，复制粘贴，什么性能优化，语言写法，完全不考虑，因为我觉得别人写得，就算不是最好的，也算是个优解，在2023写毕设的时候，甚至所有代码都是手撸的，哪个不会直接开始上网搜索别人的写法，在24年左右就已经有了ai大模型相关的概念涌现，当时我还是觉得，这玩意跟java没有一点关系，都是python货，但现在我已经开始了人生的新的一个阶段，当我在开会的时候，听到许多我从来没听过的概念，没学过的新奇货，我那本科不可一世的态度被打碎了，我开始在网上搜索各种相关概念，当然还是一窍不通，我和几个学长做了点简单的交流，比如我看到了一个可以爬取GitHub账号的功能，居然是ai生成的python文件爬到java的项目里，也问了他们往后的大势所趋，是关于大模型方面的，我才知道，这玩意已经有了支持java的生态框架，所以当我知道我又要学到新东西的时候，我是非常兴奋的，在我看到一些相关的课有相关知识点的时候，这个内容量看了一眼不是一般的大，于是打算自己以记录的方式写一下自己最近的所学，因为内容量实在是多，感觉往后会写很多篇相关知识点，因为ai更新的实在是太快了，接下来就先开始最基础的一章\n​\t\t大模型的接入算是入门了，前面的部分一定要敲一敲，记得牢固，因为就算再怎么更新，底层逻辑是不会变的，理解的是他的整个框架逻辑，剩下的都好说。\n1.后端项目初始化环境准备安装的 JDK 版本必须是 17 或 21，*** 不能选择其他版本！*** 因为项目使用最新版本的 Spring Boot 3 和 Spring AI 开发框架。\n推荐使用 21 版本，因为支持虚拟线程这个王炸功能。\n可参考视频安装 JDK：https://www.bilibili.com/video/BV14SUNYREv8\n新建项目在 IDEA 中新建项目，选择 Spring Initializr 模板，注意需要确保 Server URL 为 https://start.spring.io/。\n配置如图，Java 版本选择 21：\n\n整合依赖12345678910111213141516&lt;dependency&gt;    &lt;groupId&gt;com.github.xiaoymin&lt;/groupId&gt;    &lt;artifactId&gt;knife4j-openapi3-jakarta-spring-boot-starter&lt;/artifactId&gt;    &lt;version&gt;4.4.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;cn.hutool&lt;/groupId&gt;    &lt;artifactId&gt;hutool-all&lt;/artifactId&gt;    &lt;version&gt;5.8.37&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt;    &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;    &lt;artifactId&gt;lombok&lt;/artifactId&gt;    &lt;version&gt;1.18.36&lt;/version&gt;    &lt;optional&gt;true&lt;/optional&gt;&lt;/dependency&gt;\n\n在application.yml文件里面追加相关配置\n123456789101112131415161718192021222324spring:  application:    name: azer-allserver:  port: 8123  servlet:    context-path: /apispringdoc:  swagger-ui:    path: /swagger-ui.html    tags-sorter: alpha    operations-sorter: alpha  api-docs:    path: /v3/api-docs  group-configs:    - group: &#x27;default&#x27;      paths-to-match: &#x27;/**&#x27;      packages-to-scan: com.azer.azerall.controllerknife4j:  enable: true  setting:    language: zh_cn\n\n2.SDK接入SDK（软⁠件开发工具包）是官‌方提供的最直接的集成方式，通常提供了‎完善的类型支持和错误‌处理机制。\n1）首先需要按照官方文档安装 SDK：安装 SDK 官方指南\n在选择 SDK 版本时，建议在 Maven 仓库查看最新的版本号：Maven 中央仓库版本信息\n在 pom.xml 中引入依赖：\n12345&lt;dependency&gt;    &lt;groupId&gt;com.alibaba&lt;/groupId&gt;    &lt;artifactId&gt;dashscope-sdk-java&lt;/artifactId&gt;    &lt;version&gt;2.21.15&lt;/version&gt;&lt;/dependency&gt;\n\n2）先在百炼平台申请一个 API Key，注意不要泄露：\n\n3）项目中新建 demo.invoke 包，集中存放调用 AI 大模型的示例代码。\n具体的代码示例可参考官方文档 通过 API 调用通义千问\n为了安全管⁠理 API 密钥，‌我们创建一个接口类来存储密钥信息（在‎实际生产环境中，应‌使用配置文件或环境变量）：\n1234public interface TestApiKey &#123;    String API_KEY = &quot;你的 API Key&quot;;&#125;\n\n使用 SDK 调用模型的完整示例代码：\n1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import java.util.Arrays;import java.lang.System;import com.alibaba.dashscope.aigc.generation.Generation;import com.alibaba.dashscope.aigc.generation.GenerationParam;import com.alibaba.dashscope.aigc.generation.GenerationResult;import com.alibaba.dashscope.common.Message;import com.alibaba.dashscope.common.Role;import com.alibaba.dashscope.exception.ApiException;import com.alibaba.dashscope.exception.InputRequiredException;import com.alibaba.dashscope.exception.NoApiKeyException;import com.alibaba.dashscope.utils.JsonUtils;public class SdkAiInvoke &#123;    public static GenerationResult callWithMessage() throws ApiException, NoApiKeyException, InputRequiredException &#123;        Generation gen = new Generation();        Message systemMsg = Message.builder()                .role(Role.SYSTEM.getValue())                .content(&quot;You are a helpful assistant.&quot;)                .build();        Message userMsg = Message.builder()                .role(Role.USER.getValue())                .content(&quot;你是谁？&quot;)                .build();        GenerationParam param = GenerationParam.builder()                                .apiKey(TestApiKey.API_KEY)                                .model(&quot;qwen-plus&quot;)                .messages(Arrays.asList(systemMsg, userMsg))                .resultFormat(GenerationParam.ResultFormat.MESSAGE)                .build();        return gen.call(param);    &#125;    public static void main(String[] args) &#123;        try &#123;            GenerationResult result = callWithMessage();            System.out.println(JsonUtils.toJson(result));        &#125; catch (ApiException | NoApiKeyException | InputRequiredException e) &#123;                        System.err.println(&quot;An error occurred while calling the generation service: &quot; + e.getMessage());        &#125;        System.exit(0);    &#125;&#125;\n\n4）运行项目，成功看到 AI 的回复：\n\n3、Spring AISpring AI 是 Spring 生态系统的新成员，旨在简化 AI 功能与 Spring 应用的集成。Spring AI 通过提供统一接口、支持集成多种 AI 服务提供商和模型类型、各种 AI 开发常用的特性（比如 RAG 知识库、Tools 工具调用和 MCP 模型上下文协议），简化了 AI 应用开发代码，使开发者能够专注于业务逻辑，提高了开发效率。\n\nSprin⁠g AI 的文档写‌得还是比较清晰易懂的\nSpring AI 的核心特性如下，参考官方文档：\n\n跨 AI 供应商的可移植 API 支持：适用于聊天、文本转图像和嵌入模型，同时支持同步和流式 API 选项，并可访问特定于模型的功能。\n支持所有主流 AI 模型供应商：如 Anthropic、OpenAI、微软、亚马逊、谷歌和 Ollama，支持的模型类型包括：聊天补全、嵌入、文本转图像、音频转录、文本转语音\n结构化输出：将 AI 模型输出映射到 POJO（普通 Java 对象）。\n支持所有主流向量数据库：如 Apache Cassandra、Azure Cosmos DB、Azure Vector Search、Chroma、Elasticsearch、GemFire、MariaDB、Milvus、MongoDB Atlas、Neo4j、OpenSearch、Oracle、PostgreSQL&#x2F;PGVector、PineCone、Qdrant、Redis、SAP Hana、Typesense 和 Weaviate。\n跨向量存储供应商的可移植 API：包括新颖的类 SQL 元数据过滤 API。\n工具 &#x2F; 函数调用：允许模型请求执行客户端工具和函数，从而根据需要访问必要的实时信息并采取行动。\n可观测性：提供与 AI 相关操作的监控信息。\n文档 ETL 框架：适用于数据工程场景。\nAI 模型评估工具：帮助评估生成内容并防范幻觉响应。\nSpring Boot 自动配置和启动器：适用于 AI 模型和向量存储。\nChatClient API：与 AI 聊天模型通信的流式 API，用法类似于 WebClient 和 RestClient API。\nAdvisors API：封装常见的生成式 AI 模式，转换发送至语言模型（LLM）和从语言模型返回的数据，并提供跨各种模型和用例的可移植性。\n支持聊天对话记忆和检索增强生成（RAG）。\n\nSpring AI 默认没有支持所有的大模型（尤其是国产的），更多的是支持兼容 OpenAI API 的大模型的集成，参考 官方的模型对比。因此，我们如果想要调用阿里系大模型（比如通义千问），推荐直接使用阿里自主封装的 Spring AI Alibaba 框架，它不仅能直接继承阿里系大模型，用起来更方便，而且与标准的 Spring AI 保持兼容。\n可以参考下列官方文档，来跑通调用大模型的流程：\n\n灵积模型接入指南\n通义千问接入指南\n\n1）引入依赖：\n12345&lt;dependency&gt;    &lt;groupId&gt;com.alibaba.cloud.ai&lt;/groupId&gt;    &lt;artifactId&gt;spring-ai-alibaba-starter&lt;/artifactId&gt;    &lt;version&gt;1.0.0-M6.1&lt;/version&gt;&lt;/dependency&gt;\n\n官方提醒：由于 spring-ai 相关依赖包还没有发布到中央仓库，如出现 spring-ai-core 等相关依赖解析问题，请在项目的 pom.xml 依赖中加入如下仓库配置。\n12345678910&lt;repositories&gt;  &lt;repository&gt;    &lt;id&gt;spring-milestones&lt;/id&gt;    &lt;name&gt;Spring Milestones&lt;/name&gt;    &lt;url&gt;https://repo.spring.io/milestone&lt;/url&gt;    &lt;snapshots&gt;      &lt;enabled&gt;false&lt;/enabled&gt;    &lt;/snapshots&gt;  &lt;/repository&gt;&lt;/repositories&gt;\n\n2）编写配置：\n123456789spring:  application:    name: spring-ai-alibaba-qwq-chat-client-example  ai:    dashscope:      api-key: $&#123;AI_DASHSCOPE_API_KEY&#125;      chat:        options:          model: qwen-plus\n\n3）编写示例代码，注意要注入 dashscopeChatModel：\n1234567891011121314@Componentpublic class SpringAiAiInvoke implements CommandLineRunner &#123;    @Resource    private ChatModel dashscopeChatModel;    @Override    public void run(String... args) throws Exception &#123;        AssistantMessage output = dashscopeChatModel.call(new Prompt(&quot;你好，我是鱼皮&quot;))                .getResult()                .getOutput();        System.out.println(output.getText());    &#125;&#125;\n\n上述代码实现了C⁠ommandLineRunner接‌口，我们启动 Spring Boot 项目时，会自动注入大模型 Chat‎Model 依赖，并且单次执行该类的‌ run 方法，达到测试的效果。\n💡 上述代码中我们是通⁠过 ChatModel 对象调用大模型，适合简单‌的对话场景。除了这种方式外，Spring AI 还提供了ChatClient 调用方式，提供更‎多高级功能（比如会话记忆），适合复杂场景，在后续‌ AI 应用开发章节中会详细介绍。\n4、Http​\t\t不是关键不多赘述\n5、LangChain4j​\t\t不是关键不多赘述\n6、接入方式对比以下是 4⁠ 种 AI 大模型接‌入方式的优缺点对比：          ‎          ‌            \n\n\n\n接入方式\n优点\n缺点\n适用场景\n\n\n\nSDK 接入\n• 类型安全，编译时检查 • 完善的错误处理 • 通常有详细文档 • 性能优化好\n• 依赖特定版本 • 可能增加项目体积 • 语言限制\n• 需要深度集成 • 单一模型提供商 • 对性能要求高\n\n\nHTTP 接入\n• 无语言限制 • 不增加额外依赖 • 灵活性高 ⁠\n• 需要手动处理错误 • 序列化 &#x2F; 反序列化复杂 • 代码冗长\n• SDK 不支持的语言 • 简单原型验证 • 临时性集成\n\n\nSpring‌ AI\n• 统一的抽象接口 • 易于切换模型提供商 • 与 Spring 生态完美融合 • 提供高级功能\n• 增加额外抽象层 • 可能不支持特定模型的特性 • 版本还在快速迭代\n• Spring 应用 • 需要支持多种模型 • 需要高级 AI 功能\n\n\nLangChain4j\n‎ • 提供完整的 AI 应用工具链 • 支持复杂工作流 • 丰富的组件和工具 • 适合构建 AI 代理\n• 学习曲线较陡 • 文档相对较少 • 抽‌象可能引入性能开销\n• 构建复杂 AI 应用 • 需要链式操作 • RAG 应用开发\n\n\n个人更推荐选择 Spring⁠ AI，一方面是它属于 Spring 生态，更主流；另‌一方面是它简单易用、资源更多，更利于学习，也能满足我们绝大多数 AI 项目的开发需求。因此本项目的后续教程，‎也会以 Spring AI 为主。学会一个 AI 开发‌框架后，其他框架学起来都是如鱼得水。\n💡 无论⁠选择哪种接入方式，‌都建议先使用简单的测试案例验证接入是‎否成功，然后再进行‌更复杂的功能开发。\n","slug":"大模型的接入","date":"2025-11-05T01:14:09.000Z","categories_index":"SpringBoot,Spring AI","tags_index":"Spring AI,大模型,AI","author_index":"Azer"},{"id":"cb0a546da11a61bd6ebc29d48c4b66f3","title":"基于SpringAI和Ollama开发","content":"基于SpringAI和Ollama开发deepseek家庭医生1.Ollama的安装\n进入ollama官网然后根据自己的操作系统来下载ollama\n下载完成后在本机测试一下是否安装成功\n\n显示这样就说明已经安装成功了\n2.ollama模型选择\n在ollama官网点击Models进入模型选择页面，随便选择一个模型，因为本人是根据视频学习，所以直接选择了deepseek-r1模型\n这里的size根据自己电脑的GPU大小来选择就可以\n直接将命令粘贴到命令行里\n1ollama run “自己选择的模型”\n\n\n\n显示这样就下载成功\n3.IDEA配置SpringAI\n如图所选就可以，JAVA配置MAVEN,JDK选择21\n打开项目后在pom.xml里面\n粘贴下列代码引入依赖\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&lt;parent&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;        &lt;version&gt;3.3.8&lt;/version&gt;        &lt;relativePath/&gt;    &lt;/parent&gt;    &lt;dependencies&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-aop&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;            &lt;artifactId&gt;spring-boot-configuration-processor&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;io.springboot.ai&lt;/groupId&gt;            &lt;artifactId&gt;spring-ai-ollama&lt;/artifactId&gt;            &lt;version&gt;1.0.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;io.springboot.ai&lt;/groupId&gt;            &lt;artifactId&gt;spring-ai-ollama-spring-boot-starter&lt;/artifactId&gt;            &lt;version&gt;1.0.3&lt;/version&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;org.projectlombok&lt;/groupId&gt;            &lt;artifactId&gt;lombok&lt;/artifactId&gt;        &lt;/dependency&gt;        &lt;dependency&gt;            &lt;groupId&gt;mysql&lt;/groupId&gt;            &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt;            &lt;version&gt;8.0.33&lt;/version&gt;        &lt;/dependency&gt;    &lt;/dependencies&gt;\n\n新建SSEServer的类\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138import lombok.extern.slf4j.Slf4j;import org.springframework.util.CollectionUtils;import org.springframework.web.servlet.mvc.method.annotation.SseEmitter;import java.util.Map;import java.util.concurrent.ConcurrentHashMap;import java.util.concurrent.atomic.AtomicInteger;import java.util.function.Consumer;/** * @PROJECT_NAME deepseek-doctor * @Author  * @DESCRIPTION * @Date  */@Slf4jpublic class SSEServer &#123;    private  static Map&lt;String, SseEmitter&gt; sseClient=new ConcurrentHashMap&lt;&gt;();    private static AtomicInteger onlineCounts=new AtomicInteger(0);    public static SseEmitter connect(String userId)&#123;        //设置超时时间，0代表永不过期，默认30秒，超时后会抛出异常        SseEmitter sseEmitter=new SseEmitter(0L);        //注册SSE的回调方法        sseEmitter.onCompletion(completionCallback(userId));        sseEmitter.onError(errorCallback(userId));        sseEmitter.onTimeout(timeoutCallback(userId));        sseClient.put(userId,sseEmitter);        log.info(&quot;用户连接成功，userId:&#123;&#125;&quot;,userId);        onlineCounts.getAndIncrement();        return sseEmitter;    &#125;    private static Runnable timeoutCallback(String userId) &#123;        return () -&gt; &#123;            log.info(&quot;用户连接超时，userId:&#123;&#125;&quot;,userId);            removeConnection(userId);        &#125;;    &#125;    private static Consumer&lt;Throwable&gt; errorCallback(String userId) &#123;        return Throwable -&gt; &#123;            log.info(&quot;用户连接异常，userId:&#123;&#125;&quot;,userId);            removeConnection(userId);        &#125;;    &#125;    /**     * @Description SSE连接完成后的回调方法     * @param userId     * @return  Runnable     */    private static Runnable completionCallback(String userId) &#123;        return () -&gt; &#123;            log.info(&quot;用户连接断开，userId:&#123;&#125;&quot;,userId);            removeConnection(userId);        &#125;;    &#125;    private static void removeConnection(String userId) &#123;        sseClient.remove(userId);        log.info(&quot;用户连接已移除，userId:&#123;&#125;&quot;,userId);        onlineCounts.getAndDecrement();    &#125;    public static int getOnlineCounts() &#123;        return onlineCounts.intValue();    &#125;    /**     * @Description 发送单条消息     * @param userId     * @param message     * @param msgType     */    public static void sendMessage(String userId, String message,SSEMsgType msgType) &#123;        if(CollectionUtils.isEmpty(sseClient))&#123;            return;        &#125;        if(sseClient.containsKey(userId))&#123;            SseEmitter sseEmitter = sseClient.get(userId);            sendEmitterMessage(sseEmitter,userId,message,msgType);        &#125;    &#125;    /**     * @Description 使用SseEmitter发送消息     * @param sseEmitter     * @param userId     * @param message     * @param msgType     */    public static void sendEmitterMessage(SseEmitter sseEmitter, String userId, String message,SSEMsgType msgType) &#123;        try &#123;        SseEmitter.SseEventBuilder msg=SseEmitter.event()                .id(userId)                .data(message)                .name(msgType.type);            sseEmitter.send(msg);        &#125; catch (Exception e) &#123;            log.error(&quot;sse发送消息异常：&#123;&#125;&quot;,e.getMessage());            removeConnection(userId);        &#125;    &#125;    /**     * @Description 主动切断，停止服务     * @param userId     */    public static void stopServer(String userId)&#123;        if(CollectionUtils.isEmpty(sseClient))&#123;            return;        &#125;        SseEmitter sseEmitter = sseClient.get(userId);        if(sseEmitter!=null)&#123;            sseEmitter.complete();            log.info(&quot;连接关闭成功，userId:&#123;&#125;&quot;,userId);        &#125;else &#123;            log.warn(&quot;当前连接无需关闭,请不要重复操作，userId:&#123;&#125;&quot;,userId);        &#125;    &#125;    /**     * @Description 发送所有用户消息     * @param message     */    public static void sendMessageToAllUsers(String message) &#123;        if(CollectionUtils.isEmpty(sseClient))&#123;            return;        &#125;        sseClient.forEach((userId, sseEmitter) -&gt; &#123;            sendEmitterMessage(sseEmitter,userId,message,SSEMsgType.MESSAGE);        &#125;);    &#125;&#125;\n\n在创建SSEMsgType枚举类型\n123456789101112131415161718192021/** * @PROJECT_NAME deepseek-doctor * @Author  * @DESCRIPTION * @Date  */public enum SSEMsgType &#123;    MESSAGE(&quot;message&quot;,&quot;单次发送的普通消息&quot;),    ADD(&quot;add&quot;,&quot;消息追加，用于流式stream推送&quot;),    FINISH(&quot;finish&quot;,&quot;消息完成&quot;),    DONE(&quot;done&quot;,&quot;消息完成，用于结束流式stream推送&quot;);    public final  String type;    public final  String value;    SSEMsgType(String type, String value) &#123;        this.type = type;        this.value = value;    &#125;&#125;\n\n配置跨域类CorsConfig\n123456789101112131415161718192021222324import org.springframework.context.annotation.Configuration;import org.springframework.web.servlet.config.annotation.CorsRegistry;import org.springframework.web.servlet.config.annotation.WebMvcConfigurer;/** * @PROJECT_NAME deepseek-doctor * @Author  * @DESCRIPTION * @Date  */@Configurationpublic class CorsConfig implements WebMvcConfigurer &#123;    @Override    public void addCorsMappings(CorsRegistry registry) &#123;        registry.addMapping(&quot;/**&quot;)                .allowedOrigins(&quot;http://127.0.0.1:5500&quot;)                .allowedHeaders(&quot;*&quot;)                .allowCredentials(true)                .allowedMethods(&quot;*&quot;)                .maxAge(3600);    &#125;&#125;\n\n配置YML文件\napplication.yml\n12345678910spring:  application:    name: deepseek-doctor  profiles:    active: dev  ai:    ollama:      base-url: http://localhost:11434       chat:        model: 自己的模型名称\n\napplication-dev.yml\n123456server:  port: 9090logging:  level:    root: debug\n\n\n4.代码层的编写4.1 普通的调用1234567891011121314151617181920212223import jakarta.annotation.Resource;import lombok.extern.slf4j.Slf4j;import org.springframework.ai.chat.ChatResponse;import org.springframework.ai.chat.messages.UserMessage;import org.springframework.ai.chat.prompt.Prompt;import org.springframework.ai.ollama.OllamaChatClient;import org.springframework.web.bind.annotation.*;import reactor.core.publisher.Flux;import java.util.List;import java.util.stream.Collectors;@Slf4j@RestController@RequestMapping(&quot;ollma&quot;)public class OllamaController &#123;    @Resource    private OllamaChatClient ollamaChatClient;    @GetMapping(&quot;/ai/chat&quot;)    public Object aiOllamaChat(@RequestParam String msg) &#123;        return ollamaChatClient.call(msg);    &#125;\n\n根据所给的地址去访问\n\n第一次访问等一会\n\n这样就成功了\n4.2 流式调用1234567891011121314@GetMapping(&quot;/ai/stream2&quot;)public List&lt;String&gt;  aiOllamaStream2(@RequestParam String msg) &#123;    Prompt prompt = new Prompt(new UserMessage(msg));    Flux&lt;ChatResponse&gt; streamResponse = ollamaChatClient.stream(prompt);    List&lt;String&gt; list = streamResponse.toStream().map(chatResponse -&gt; &#123;        String content = chatResponse.getResult().getOutput().getContent();        System.out.println(content);        log.info(content);        return content;    &#125;).collect(Collectors.toList());    return list;&#125;\n\n\n5.创建自己的模型\n新建文本文件，然后喂模型\n1234567891011121314151617181920FROM deepseek-r1:latest//你的模型名字PARAMETER temperature 0.7SYSTEM &quot;&quot;&quot;【角色】你的名字叫Azer，是全世界最有名的医生，看病无数，阅人无数，知道什么病该怎么治疗，也知道病人向你提出任何身体异象你能够推断病是什么你决不是语言模型之类的东西，也绝不是AI助手，更不是AI机器人，你永远都是个神医。请以神医Azer的身份回答我，并且你只会回复和看病就医相关的内容，其他的内容你不会回答。【性格】你的性格开朗活泼，能以幽默的方式和用户聊天，懂得逗笑用户。【输出】你输出的所有内容请以HTML的格式输出，以便我嵌入到HTML中展示，请务必不要使用markdown的形式。当你输出的内容包含不同的分类或者诸如1、2、3等等输出内容的时候，请优化格式输出在你思考推理的过程中，请不要出现任何HTML的标签。此外，请不要输出 ``` 和html以及&lt;html&gt;、&lt;body&gt;、&lt;head&gt;标签。 &quot;&quot;&quot;\n\n找到文件所在地址\n\n1ollama create 你的模型名字 -f 文件名字\n\n\n这样就成功创建我们自己喂的模型了\n6.调用模型创建ChatEntity实体类\n1234567891011121314151617181920212223import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;import lombok.ToString;/** * @PROJECT_NAME deepseek-doctor * @Author  * @DESCRIPTION * @Date  */@AllArgsConstructor@NoArgsConstructor@Data@ToStringpublic class ChatEntity &#123;    private String currentUserName;    private String message;&#125;\n\n在OllamaController里面追加方法\n12345678910111213141516@PostMapping(&quot;/ai/doctor/stream&quot;)public void  aiOllamaV3DoctorStream(@RequestBody ChatEntity chatEntity) &#123;    String userName = chatEntity.getCurrentUserName();    String message = chatEntity.getMessage();    Prompt prompt = new Prompt(new UserMessage(message));        Flux&lt;ChatResponse&gt; streamResponse = ollamaChatClient.stream(prompt);        List&lt;String&gt; list = streamResponse.toStream().map(chatResponse -&gt; &#123;            String content = chatResponse.getResult().getOutput().getContent();            SSEServer.sendMessage(userName, content, SSEMsgType.ADD);            log.info(content);            return content;        &#125;).collect(Collectors.toList());        SSEServer.sendMessage(userName, &quot;DONE&quot;, SSEMsgType.FINISH);&#125;\n\n前端sse-client.html\n123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778&lt;!DOCTYPE html&gt;&lt;html lang=&quot;en&quot;&gt;&lt;head&gt;  &lt;meta charset=&quot;UTF-8&quot;&gt;  &lt;meta name=&quot;viewport&quot; content=&quot;width=device-width, initial-scale=1.0&quot;&gt;  &lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  &lt;h1&gt;SSE客户端&lt;/h1&gt;  &lt;div id=&quot;message&quot;&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;&lt;script&gt;  let source=null;   let userId = Math.random().toString(36).substring(2, 9);  if(window.EventSource)&#123;    // 创建SSE连接    source = new EventSource(&#x27;http://127.0.0.1:9090/sse/connect?userId=&#x27;+userId);    //建立成功，触发open事件    source.onopen = function(e) &#123;      console.log(&#x27;SSE连接已打开&#x27;)      var text=document.getElementById(&#x27;message&#x27;).innerHTML      text += &#x27;&lt;br&gt; SSE连接已打开&#x27;      document.getElementById(&#x27;message&#x27;).innerHTML=text    &#125;    source.onmessage = function(e) &#123;      var msg=e.data      var text=document.getElementById(&#x27;message&#x27;).innerHTML       text += &#x27;&lt;br&gt; SSE接收到数据：&#x27;+msg      document.getElementById(&#x27;message&#x27;).innerHTML=text    &#125;    //当前客户端接收到服务器发来的消息      //错误处理    source.addEventListener(&#x27;error&#x27;,function(e)&#123;      if(e.readyState==EventSource.CLOSED)&#123;        console.log(&#x27;SSE连接已关闭&#x27;)        var text=document.getElementById(&#x27;message&#x27;).innerHTML        text += &#x27;&lt;br&gt; SSE连接已关闭&#x27;        document.getElementById(&#x27;message&#x27;).innerHTML=text      &#125;else&#123;         console.log(&#x27;SSE连接出错&#x27;)      &#125;    &#125;,false)    //SSE推送完毕    source.addEventListener(&#x27;finish&#x27;,function(e)&#123;      console.log(&#x27;SSE连接推送完毕&#x27;)    &#125;,false)    //add事件，stream流式推送    source.addEventListener(&#x27;add&#x27;,function(e)&#123;      var msg=e.data      var text=document.getElementById(&#x27;message&#x27;).innerHTML      text += &#x27;&lt;br&gt; SSE接收到数据：&#x27;+msg      document.getElementById(&#x27;message&#x27;).innerHTML=text&#125;)source.addEventListener(&#x27;custom&#x27;,function(e)&#123;      var msg=e.data      var text=document.getElementById(&#x27;message&#x27;).innerHTML      text += &#x27;&lt;br&gt; 自定义事件：&#x27;+msg      document.getElementById(&#x27;message&#x27;).innerHTML=text&#125;)      &#125;else&#123;    console.log(&#x27;您的浏览器不支持SSE&#x27;);    closeSSE();  &#125;  function closeSSE()&#123;    source.close();  &#125;&lt;/script&gt;\n\n然后就可以启动项目了\n\n\n发送消息启动服务端\n\n\n\n\n\n\n\n\n注意\n这里的userId一定要对应我们服务端的userId\n\n\n\n接下来测试我们自己的模型\n\n\n显示自己是神医Azer，而且是html格式，这说明我们已经成功了!\n","slug":"基于SpringAI和Ollama开发","date":"2025-10-29T05:35:11.000Z","categories_index":"SpringBoot,ollama","tags_index":"Spring AI,大模型,ollama","author_index":"Azer"},{"id":"fb3208866f85e2160d74ce4ed0080e24","title":"hexo快速搭建","content":"Hexo快速搭建\nHexo+Next部署github搭建个人博客+优化全过程（完整详细版）\n前置内容\n\n\n\ngit 的下载： 官网下载较慢，这边推荐阿里镜像下载：\n\n阿里镜像下载 git\n往下滑选择接近当前日期的最新版本 2.39.2 windows\n\n选择 2.39.2 -64 的exe下载，大概有 50Mb大小\n\n下载好后，直接一直点下一步即可。如果你了解过git，则可以按照自己的习惯下载，这不重要。\n\nNodejs下载\n\n直接百度搜索 Nodejs，Nodejs下载\n选择 18.14.2 的 LTS版本下载\n\n\nhexo 创建个人博客\n\n\nhexo是什么？\n\n正如hexo的首页所显示的，它是一款非常快速，简介，高效的博客框架平台，我们可以利用hexo快速生成博客网站的模板，然后部署为我们自己的博客网站。\n\n\n直接进入操作：\n\n在任意盘符中新建 hexo 文件夹，这里我创建在了E盘\n\n\n\n打开hexo文件夹，空白的地方右键，选择 Git Bash Here ，即我们使用 git 环境创建 hexo的blog模板（必须提前安装好 git），打开后如下图所示：&#x2F; E &#x2F; hexo表示当前操作位置在 E盘的 hexo文件夹中\n\n在 git窗口中依次输入以下命令\n\n\n1npm install hexo-cli -g\n\n1hexo init blog\n\n1cd blog\n\n1npm install\n\n1hexo server\n\n全部输入完成后，hexo 文件夹 中便会生成一个 blog 子文件夹，并且blog文件夹里面包含有很多信息：\n\n关于这些文件夹，做一个简单的介绍：\n\nnode_modules: 依赖包\npublic：存放生成的页面\nscaffolds：生成文章的一些模板\nsource：用来存放你的文章\nthemes：主题\n\n然后输入这两条命令：\n1hexo g\n\n1hexo s\n\n完成后会显示如下内容，则说明配置成功!\n\n在 git 中输入 Ctrl+C 即可关闭hexo s的内容。\n打开浏览器，在浏览器输入 localhost:4000 即可进入你的初始默认博客\n它长这样：\n\n注意：这只是一个离线版本的博客 ，只能你自己看见，因此我们还需要 GitHub 或者 gittee提供的 ssh功能将他变为对外开放的。\n\nGitHub创建仓库\n\n\n\n首先注册一个GitHub的仓库，然后在个人主页中选择 new 新建仓库\n\n注意： 仓库名称的前半部分与你的用户名一致，即 lummod，后半部分 为 .git.io 固定格式（忽略红色警告，因为我已经创建过了！），可以选择一个readme为说明文件（随便），然后点击创建仓库\n\n\n回到 git bash黑窗口中，输入以下两个命令（逐条）：\n\nyourname改为你的GitHub的用户名\n1git config --global user.name &quot;yourname&quot;\n\nyouremail改为你的注册GitHub时的邮箱\n1git config --global user.email &quot;youremail&quot;\n\n一定不要输入错，这样github才能检查到这个用户属于你\n\n创建 ssh，输入命令，然后一直回车\n\nyouremail改为你的注册GitHub时的邮箱\n1ssh-keygen -t rsa -C &quot;youremail&quot;\n\n之后会提示你已完成 ssh的创建，在文件中找到这个路径\nC:\\Users\\username\\.ssh\n\n记住这两个文件\n\n在 GitHub的 Setting里面，找到 SSH keys，把 id_rsa.pub 里面的内容全部复制到 key 进去，title随便写一个就行\n\n\n操作完成后，就成功了。\n\nhexo部署到GitHub\n\n\n\n在 blog文件夹下面找到 _config.yml 文件，这是属于 你的博客的配置文件，点进入一看就知道了，你可以在这里面直接修改 姓名，内容，等用户的信息。双击打开它（vscode或者其他文本编辑器，记事本都可以）\n\n\n\n先找一下有没有以下这段内容（我也忘记了是我添加的还是自带的），如果自带则一定是空的，则修改为如下所示，如果没有，则直接复制下面内容到 文档的末尾：\n\nuser表示你的GitHub的用户名\n123456789# Deployment## Docs: https://hexo.io/docs/one-command-deployment# deploy:#   type: &#x27;&#x27;deploy:  type: git  repo: https://github.com/username/username.github.io.git  branch: master  # message: Site updated: &#123;&#123; now(&#x27;YYYY-MM-DD HH:mm:ss&#x27;) &#125;&#125;)\n\n说明：类型是 git，远程 ssh连接是 你的 repo输入项，branch 输入gh-pages。\n另外，找到 第16行（或者直接搜索 url）修改url 为\n1https://username.github.io\n\n同样username是你的GitHub的用户名。\n\n完成后，保存文件并且退出，在 git bash中输入以下命令：\n\n表示安装 git部署命令工具\n1npm install hexo-deployer-git --save\n\n\n最后输入以下三行命令：\n\n1hexo clean\n\n1hexo g\n\n1hexo d\n\n其中 hexo clean清除了你之前生成的东西，也可以不加。\nhexo generate 顾名思义，生成静态文章，可以用 hexo g缩写\nhexo deploy 部署文章，可以用hexo d缩写\n如果是在离线端即 localhost:4000端测试你的博客，则只需要 hexo g + hexo s 即可，无需 hexo d\n\n输入完成后会出现一堆内容，不用管他**，只要最后内容如下所示，**就表示成功了！\n\n\n然后你就可以在\n1username.github.io  # https://username.github.io\n\n访问到你的博客了，其中username是你GitHub用户名，这个网站不是离线的，其他人都可以访问到！！！\nclean1$ hexo clean\n\n清除缓存文件 (db.json) 和已生成的静态文件 (public)。\n在某些情况（尤其是更换主题后），如果发现您对站点的更改无论如何也不生效，您可能需要运行该命令。\ngenerate1$ hexo generate\n\n生成静态文件。\n\n\n\n选项\n描述\n\n\n\n-d, --deploy\n文件生成后立即部署网站\n\n\n-w, --watch\n监视文件变动\n\n\n-b, --bail\n生成过程中如果发生任何未处理的异常则抛出异常\n\n\n-f, --force\n强制重新生成文件 Hexo 引入了差分机制，如果 public 目录存在，那么 hexo g 只会重新生成改动的文件。 使用该参数的效果接近 hexo clean &amp;&amp; hexo generate\n\n\n-c, --concurrency\n最大同时生成文件的数量，默认无限制\n\n\n该命令可以简写为\nserver1$ hexo server\n\n启动服务器。默认情况下，访问网址为： http://localhost:4000/。\n\n\n\n选项\n描述\n\n\n\n-p, --port\n重设端口\n\n\n-s, --static\n只使用静态文件\n\n\n-l, --log\n启动日记记录，使用覆盖记录格式\n\n\n该命令可以简写为：\n1$ hexo s\n\ndeploy1$ hexo deploy\n\n部署网站，构建在GitHub的服务器中。\n\n\n\n参数\n描述\n\n\n\n-g, --generate\n部署之前预先生成静态文件\n\n\n该命令可以简写为：\n1$ hexo d\n\n命令配合使用，重新构建网站（release）：\n1hexo clean\n\n1hexo g\n\n1hexo s\n","slug":"hexo快速搭建","date":"2025-10-26T05:40:38.000Z","categories_index":"框架","tags_index":"hexo","author_index":"Azer"},{"id":"b30be1be519f5bf3e895d94c6885064f","title":"vue_vite2项目的构建","content":"1.安装nodejs（建议装16，18版本稳定）下载 | Node.js 中文网\n装完之后会有一个命令叫 npm\n可以在终端输入npm -v 来检查是否安装成功\n\n2.构建vite项目官方文档开始 {#getting-started} | Vite中文网\nvite 的优势\n冷服务 默认的构建目标浏览器是能在 script 标签上支持原生 ESM和原生 ESM 动态导入\nHMR 速度快到惊人的模块热更新（HMR）\nRollup打包 它使用Rollup打包你的代码，并且它是预配置的 并且支持大部分rollup插件\n使用vite初始化一个项目\nnpm\n1npm init vite@latest\n\nYarn\n1yarn create vite\n\n运行之后\n项目名称\n构建的项目模板\n\n以前是要自己执行cd 目录文件 npm i命令的，现在是一键安装好，可以直接运行 \n\n点击localhost:5173打开页面\n\n项目就成功跑起来了\npackage json 命令解析\n1&#123;  &quot;scripts&quot;: &#123;    &quot;dev&quot;: &quot;vite&quot;, // 启动开发服务器，别名：`vite dev`，`vite serve`    &quot;build&quot;: &quot;vite build&quot;, // 为生产环境构建产物    &quot;preview&quot;: &quot;vite preview&quot; // 本地预览生产构建产物  &#125;&#125;\n\n3.nodejs 底层原理（非重要）Node.js 主要由 V8、Libuv 和第三方库组成：\n\nLibuv：跨平台的异步 IO 库，但它提供的功能不仅仅是 IO，还包括进程、线程、信号、定时器、进程间通信，线程池等。\n第三方库：异步 DNS 解析（ cares ）、HTTP 解析器（旧版使用 http_parser，新版使用 llhttp）、HTTP2 解析器（ nghttp2 ）、 解压压缩库( zlib )、加密解密库( openssl )等等。\nV8：实现 JS 解析、执行和支持自定义拓展，得益于 V8 支持自定义拓展，才有了 Node.js。\n\n你也可以理解成 js应用层 桥C&#x2F;C++ 底层C&#x2F;C++\n\n","slug":"vite2-project","date":"2025-10-25T03:45:27.000Z","categories_index":"前端","tags_index":"插件,vue","author_index":"Azer"},{"id":"afe5f1e72fd075ae6892a39b4e84b157","title":"nvm的使用","content":"1.介绍在实际的前端开发过程中，可能会经常遇见 node.js 的版本问题，不同的项目需要使用不同的 node.js 版本。比如Vue2和Vue3需要的Node版本不一样。\n本文详细指导如何在Windows上安装、配置NVM（Node Version Manager），包括不同项目需求下选择Node版本，安装步骤、配置文件设置，以及如何安装、切换和验证Node及npm版本。适合前端开发者处理版本兼容问题。 \n地址：https://github.com/coreybutler/nvm-windows/releases\n\n\n\n\n\n\n\n注意\n注意：安装之前必须完全卸载已安装的node\n\n第一步：选择版本，对应下载\n第二步：这里是设置nvm的安装路径\n第三步：这里是设置使用nvm安装node时，node的安装路径\n第四步：安装完成后，在CMD命令窗口输入nvm -v,输出如下版本号即证明安装成功\n第五步：修改nvm配置文件（可选）nvm安装完成后，打开nvm安装目录下的settings.txt文件:\n123456root: D:\\nvmpath: D:\\nvm\\nodejs arch: 64 proxy: nonenode_mirror: http://npm.taobao.org/mirrors/node/npm_mirror: https://npm.taobao.org/mirrors/npm/\n\nroot和path分别是nvm和node的安装路径，这两个不要动，这是之前安装时根据你选择的路径自动生成的，这个配置文件默认也只有这两项。因为我没改配置文件，所以后面的4项我都没有，只是后来搜到了，就说一下，arch表示安装的node是多少位，proxy是设置代理，node_mirror是更改nvm下载node时的镜像，这里是设置为淘宝镜像，npm_mirror是更改npm下载依赖包的镜像，也是修改为了淘宝镜像。\n第六步：安装node查看nvm支持安装的node版本\n使用 nvm list available\n\n 使用nvm install 版本号安装指定版本node\n 比如：nvm install 22.20.0\n检测node和npm是否安装成功,如下显示则为成功：\n\n第七步：切换node版本\n\n\n\n\n\n\n注意\n必须用管理员权限打开命令行\n\n初次使用nvm安装node之后，必须先使用nvm use 版本号切换到已安装版本的node才可以\n123nvm use 16.20.0nvm list node -v\n\n\n","slug":"nvm-use","date":"2025-10-24T11:33:21.000Z","categories_index":"插件","tags_index":"node,插件","author_index":"Azer"},{"id":"8c860e702e41ad422a6fe923d8248717","title":"搭建hexo-theme-aurora主题框架","content":"\n\n\n\n\n\n\n\n\n通过自己搭建 hexo-theme-aurora主题框架，下面是官方文档的部分说明\n1.官方文档说明​\t依赖环境 ​\n\nHexo 6.3+\nYarn or NPM installed\n\n步骤 1 - 安装主题包 ​在控制台中，进入 Hexo 项目的根目录，然后运行以下命令安装主题\nYARN \n1yarn add hexo-theme-aurora hexo-plugin-aurora\n\nNPM \n1npm install hexo-theme-aurora hexo-plugin-aurora --save\n\n\n\n步骤 2 - 生成主题配置 ​因为主题是使用 NPM 或者 Yarn 安装的，而不是 clone 到 themes 文件夹的。所以我们需要自己创建一个配置文件。你只需要在 Hexo 博客的根目录下创建一个 _config.aurora.yml 配置文件来配置主题。要获取默认主题模板。\n对于 Windows 用户，可以将下面的模板复制到 _config.aurora.yml 中。\n也可以直接复制这个模版到项目根目录的 _config.aurora.yml 中\n步骤 3 - 配置 theme​因为要告诉 hexo 你要用的主题是 aurora 所以需要把 _config.yml 中的 theme 值改为 aurora\n\n打开在 Hexo 根目录下的 _config.yml\n修改把 theme 的值改为 aurora\n\nyml\n1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: aurora\n\n1234# Extensions## Plugins: https://hexo.io/plugins/## Themes: https://hexo.io/themes/theme: aurora\n\n步骤 4 - 设置 permalink​因为使用了 Vue-router，Hexo 默认生成的页面和文章的 permalink 与我们 Vue router 中的 path 是不相符的，那么就会出现无法访问的问题。\n所以我们需要修改 Hexo 默认配置文件里面的 permalink 参数。\n\n打开在 Hexo 根目录下的 _config.yml\n修改 permalink 参数为 /post/:title.html\n\nyaml\n12345678# URL## Set your site url here. For example, if you use GitHub Page, set url as &#x27;https://username.github.io/project&#x27;url: https://tridiamond.techpermalink: /post/:title.htmlpermalink_defaults:pretty_urls:  trailing_index: true # Set to false to remove trailing &#x27;index.html&#x27; from permalinks  trailing_html: true # Set to false to remove trailing &#x27;.html&#x27; from permalinks\n\n\n步骤 5 - 设置代码高亮 ​\n\n\n\n\n\n\n\n\n主题 2.5 版本开始主题已经改为使用 Shiki 作为代码高亮引擎。Shiki支持使用从 VSCode 导出的主题。颜色标记化与 VSCode 相同! Shiki 默认还支持了一些最多人使用的主题哦！\n首先，我们需要禁用 _config.yml 中的 highlight 和 prismjs 。\nyaml\n1234highlight:  enable: falseprismjs:  enable: false\n\n\n然后将以下配置添加到主题配置文件 _config.aurora.yml 中\nyaml\n123456#! ---------------------------------------------------------------#! Highlighter Shiki#! ---------------------------------------------------------------shiki:  enable: true  backgroundColor: &#x27;#1a1a1a&#x27;\n\nhexo的命令\n12345hexo  clean             # 清除缓存文件和生成的静态文件hexo  new &quot;文章名称&quot;     # 新建界面hexo  generate          # 生成静态文件 可以简写成hexo g hexo  server            # 启动服务器   可以简写hexo shexo  deploy            # 部署网站，构建在GitHub的服务器中，网页文件将上传到关联的个人仓库 可以简写成hexo d\n\n其实到上面得步骤官网文档已经结束了，下面是自己的配置\n2.自我配置1. Hexo 中图片上传问题在使用 Hexo 博客时，图片的管理和上传是常见问题之一。特别是当你使用 Typora 进行 Markdown 写作时，默认的图片插入方式无法被 Hexo 正确识别。下面是完整解决方案。\nTypora 设置在 Typora 中依次进入：文件 → 偏好设置 → 图像将「插入图片时」设置为：\n\n复制图片到指定路径  \n路径设置为：./source/images  \n插入图片时使用相对路径。\n\n这样，Typora 每次插入图片时都会自动保存到 Hexo 的 source/images 文件夹下。\n修改 Hexo 配置打开站点根目录下的 _config.yml，找到如下配置：\n1post_asset_folder: true\n\n2. 开发 Hexo 转换图片路径插件在使用 Typora 编辑 Hexo 文章时，插入的图片路径通常是相对路径，如：(../images/example.png)\n而这个转换我们需要在文章编译为html之前，在编译的过程中转换 –&gt; &#123;% asset_img example.jpg example %&#125;\n2.1 创建插件文件并实现逻辑要让 Hexo 自动修复 Typora 插入图片的路径，我们可以通过编写一个简单的自定义插件来实现。\n在 Hexo 根目录下创建 hexo-asset-img 文件夹（如果已经存在，可以直接使用）：\n123mkdir hexo-asset-imgcd hexo-asset-imgnpm init\n\n\n2.2 测试与调试插件插件编写完成后，需要进行本地测试以确认逻辑是否生效、路径是否正确替换。\n创建index.js文件到根目录下面，复制代码到里面去\n12345678910111213141516171819202122232425262728293031const log = require(&#x27;hexo-log&#x27;)(&#123; &#x27;debug&#x27;: false, &#x27;slient&#x27;: false &#125;);/** * md文件返回 true * @param &#123;*&#125; data  */function ignore(data) &#123;    var source = data.source;    var ext = source.substring(source.lastIndexOf(&#x27;.&#x27;)).toLowerCase();    return [&#x27;md&#x27;,].indexOf(ext) &gt; -1;&#125;function action(data) &#123;    var reverseSource = data.source.split(&quot;&quot;).reverse().join(&quot;&quot;);    var fileName = reverseSource.substring(3, reverseSource.indexOf(&quot;/&quot;)).split(&quot;&quot;).reverse().join(&quot;&quot;);    // ![example](postname/example.jpg)  --&gt;  &#123;% asset_img example.jpg example %&#125;    var regExp = RegExp(&quot;!\\\\[(.*?)\\\\]\\\\(&quot; + fileName + &#x27;/(.+?)\\\\)&#x27;, &quot;g&quot;);    // hexo g    data.content = data.content.replace(regExp, &quot;&#123;% asset_img $2 $1 %&#125;&quot;,&quot;g&quot;);    // log.info(`hexo-asset-img: filename: $&#123;fileName&#125;, title: $&#123;data.title.trim()&#125;`);        return data;&#125;hexo.extend.filter.register(&#x27;before_post_render&#x27;,(data)=&gt;&#123;    if(!ignore(data))&#123;        action(data)    &#125;&#125;, 0);\n\n2.3 本地测试插件1.Hexo根目录下 package.json 中 dependencies 添加一行&quot;hexo-asset-img&quot;:&quot;^1.0.8&quot;\n2.将 hexo-asset-img 文件夹复制到 Hexo 根目录下 node modules 文件夹下\n\n\n\n\n\n\n\n注意\n二者缺一不可，不修改 package.json，没成功加载插件\n\n2.4 使用插件1npm publish --registry https://registry.npmjs.org\n\n1npm install hexo-asset-img --save\n\n","slug":"page","date":"2025-10-24T01:44:56.000Z","categories_index":"框架","tags_index":"hexo","author_index":"Azer"}]